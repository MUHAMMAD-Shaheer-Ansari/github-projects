{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MODEL BUILDING"
      ],
      "metadata": {
        "id": "DGH3dE_U-KtA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wHiUpWj-DPJ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community transformers==4.57.6 sentence-transformers faiss-cpu pypdf fastapi uvicorn pyngrok nest-asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Folder containing PDFs\n",
        "pdf_folder = \"/pdf data\"  # ensure PDFs exist here\n",
        "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
        "\n",
        "all_documents = []\n",
        "\n",
        "# Load PDFs and lowercase content\n",
        "for pdf_file in pdf_files:\n",
        "    loader = PyPDFLoader(os.path.join(pdf_folder, pdf_file))\n",
        "    docs = loader.load()\n",
        "    for doc in docs:\n",
        "        doc.page_content = doc.page_content.lower()\n",
        "    all_documents.extend(docs)\n",
        "\n",
        "print(f\"Total documents loaded: {len(all_documents)}\")\n",
        "\n",
        "# Split documents into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "chunks = splitter.split_documents(all_documents)\n",
        "\n",
        "print(f\"Total chunks created: {len(chunks)}\")\n",
        "\n",
        "# Create embeddings\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Build FAISS vector store\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "# Load FLAN-T5 model\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Build RAG chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    chain_type=\"stuff\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "RlzY_0qS-aX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query\n",
        "query = \"what is a circle\"\n",
        "query = query.lower()\n",
        "answer = qa_chain.run(query)\n",
        "\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOW0ZPPP-sCQ",
        "outputId": "9f9acbbc-0ced-4bb1-def7-08539698a47f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1596007599.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.\n",
            "  answer = qa_chain.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: the boundary of the disc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.save_local('/vector_store')"
      ],
      "metadata": {
        "id": "Sxi9W6TA-yjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEPLOYMENT (FASTAPI)"
      ],
      "metadata": {
        "id": "xEVKI374AJi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Build FAISS vector store\n",
        "vectorstore = FAISS.load_local('/vector_store', embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Load FLAN-T5 model\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Build RAG chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    chain_type=\"stuff\"\n",
        ")\n",
        "\n",
        "import asyncio\n",
        "from fastapi import FastAPI\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class StrInput(BaseModel):\n",
        "  text : str\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "def home():\n",
        "  return 'NOTHING HERE GO TO /predict'\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict_rag_response(data : StrInput):\n",
        "  return JSONResponse(content=qa_chain.run(data.text.lower()))\n",
        "\n",
        "\n",
        "''' One important note I was running this code in colab so for colab I was using ngrok\n",
        "if you are running this in your local machine then no need to use ngrok use uvicorn's standard method\n",
        "to run it on localhost. The code below is useless if you are running it on your local machine.'''\n",
        "\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set ngrok token\n",
        "ngrok.set_auth_token(\"YOUR AUTH KEY\")\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "async def uvicorn_serve():\n",
        "  config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, loop=\"asyncio\")\n",
        "  server = uvicorn.Server(config)\n",
        "  await server.serve()\n",
        "# Schedule the server to run as a task in the existing event loop\n",
        "loop = asyncio.get_event_loop()\n",
        "loop.create_task(uvicorn_serve())\n",
        ""
      ],
      "metadata": {
        "id": "yI-swvKdAFkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# Terminate any ngrok tunnels currently running\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "Oii8XVDUAyAy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iF0f7k8SW0_T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}