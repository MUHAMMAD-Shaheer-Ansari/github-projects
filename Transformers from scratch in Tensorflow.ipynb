{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE_FATa1lKL2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import MultiHeadAttention,Dense,LayerNormalization,Embedding,Layer,Input\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TI8hHD7lc_3"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(seq_len , model_size):\n",
        "  output = []\n",
        "  for pos in range(seq_len):\n",
        "    PE = np.zeros(model_size)\n",
        "    for i in range(model_size):\n",
        "      if i % 2 == 0:\n",
        "        PE[i] = np.sin(pos / (10000 ** (i/model_size)))\n",
        "      else:\n",
        "        PE[i] = np.cos(pos / (10000 ** ((i-1)/model_size)))\n",
        "\n",
        "    output.append(PE)\n",
        "    out = np.expand_dims(output , axis = 0)\n",
        "\n",
        "  return out\n",
        "\n",
        "class Embeddings(Layer):\n",
        "  def __init__(self , vocab_size , seq_len , model_size):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.emb = Embedding(input_dim = vocab_size , output_dim = model_size)\n",
        "    self.pos_encoding = positional_encoding(seq_len,model_size)\n",
        "\n",
        "  def call(self,input):\n",
        "    embs = self.emb(input)\n",
        "    return (self.pos_encoding + embs)\n",
        "\n",
        "  def compute_masks(self,input):\n",
        "    mask = tf.math.not_equal(input , 0)\n",
        "    mask = tf.cast(mask[:,tf.newaxis,:],tf.int32)\n",
        "    T = tf.shape(mask)[2]\n",
        "    mask = tf.repeat(mask , T , axis = 1)\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "925x8QLrsUzr"
      },
      "outputs": [],
      "source": [
        "class Encoderlayer(Layer):\n",
        "  def __init__(self, num_heads , emb_dim , dense_dim):\n",
        "    super(Encoderlayer,self).__init__()\n",
        "    self.layernorm_1 = LayerNormalization()\n",
        "    self.layernorm_2 = LayerNormalization()\n",
        "    self.dense = tf.keras.Sequential([\n",
        "        Dense(dense_dim,activation = 'relu'),\n",
        "        Dense(emb_dim)\n",
        "    ])\n",
        "    self.attn = MultiHeadAttention(num_heads=num_heads,key_dim=emb_dim)\n",
        "\n",
        "  def call(self,inputs , mask):\n",
        "    attn_out = self.attn(query = inputs , key = inputs , value = inputs , attention_mask = mask)\n",
        "    out = self.layernorm_1(attn_out + inputs)\n",
        "\n",
        "    dense_out = self.dense(out)\n",
        "\n",
        "    return self.layernorm_2(dense_out + out)\n",
        "\n",
        "\n",
        "class Decoderlayer(Layer):\n",
        "  def __init__(self, num_heads , emb_dim , dense_dim):\n",
        "    super(Decoderlayer,self).__init__()\n",
        "    self.layernorm_1 = LayerNormalization()\n",
        "    self.layernorm_2 = LayerNormalization()\n",
        "    self.layernorm_3 = LayerNormalization()\n",
        "\n",
        "    self.dense = tf.keras.Sequential([\n",
        "        Dense(dense_dim,activation = 'relu'),\n",
        "        Dense(emb_dim)\n",
        "    ])\n",
        "    self.attn_1 = MultiHeadAttention(num_heads=num_heads,key_dim=emb_dim)\n",
        "    self.attn_2 = MultiHeadAttention(num_heads=num_heads,key_dim=emb_dim)\n",
        "\n",
        "\n",
        "  def call(self,inputs ,encoder_outputs, mask):\n",
        "    causal_mask = tf.linalg.band_part(tf.ones([tf.shape(inputs)[0],\n",
        "                                                  tf.shape(inputs)[1],\n",
        "                                                  tf.shape(inputs)[1]],dtype = tf.int32),-1,0)\n",
        "\n",
        "    attn_mask = tf.minimum(mask , causal_mask)\n",
        "\n",
        "\n",
        "    attn_out = self.attn_1(query = inputs , key = inputs , value = inputs , attention_mask = attn_mask)\n",
        "    out_1 = self.layernorm_1(attn_out + inputs)\n",
        "\n",
        "    attn_out_2 = self.attn_2(query = out_1 ,key = encoder_outputs , value = encoder_outputs , attention_mask = None )\n",
        "    out_2 = self.layernorm_2(out_1 + attn_out_2)\n",
        "\n",
        "    dense_out = self.dense(out_2)\n",
        "\n",
        "    return self.layernorm_3(dense_out + out_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM32iX_zURvr",
        "outputId": "0be69638-5f9e-46ee-b011-802ef4256181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " tf.math.not_equal (TFOpLam  (None, None)                 0         ['input_1[0][0]']             \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (  (None, 1, None)              0         ['tf.math.not_equal[0][0]']   \n",
            " SlicingOpLambda)                                                                                 \n",
            "                                                                                                  \n",
            " tf.math.not_equal_1 (TFOpL  (None, None)                 0         ['input_2[0][0]']             \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.cast (TFOpLambda)        (None, 1, None)              0         ['tf.__operators__.getitem[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2  (None, 1, None)              0         ['tf.math.not_equal_1[0][0]'] \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape (TFOpLa  (3,)                         0         ['tf.cast[0][0]']             \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.cast_1 (TFOpLambda)      (None, 1, None)              0         ['tf.__operators__.getitem_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_1  ()                           0         ['tf.compat.v1.shape[0][0]']  \n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape_1 (TFOp  (3,)                         0         ['tf.cast_1[0][0]']           \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " embeddings (Embeddings)     (None, 2, 512)               1024000   ['input_1[0][0]']             \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " tf.repeat (TFOpLambda)      (None, None, None)           0         ['tf.cast[0][0]',             \n",
            "                                                                     'tf.__operators__.getitem_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_3  ()                           0         ['tf.compat.v1.shape_1[0][0]']\n",
            "  (SlicingOpLambda)                                                                               \n",
            "                                                                                                  \n",
            " embeddings_1 (Embeddings)   (None, 2, 512)               1024000   ['input_2[0][0]']             \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " encoderlayer (Encoderlayer  (None, 2, 512)               1050316   ['embeddings[0][0]',          \n",
            " )                                                        8          'tf.repeat[0][0]']           \n",
            "                                                                                                  \n",
            " tf.repeat_1 (TFOpLambda)    (None, None, None)           0         ['tf.cast_1[0][0]',           \n",
            "                                                                     'tf.__operators__.getitem_3[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " decoderlayer (Decoderlayer  (None, 2, 512)               1890560   ['embeddings_1[0][0]',        \n",
            " )                                                        0          'encoderlayer[0][0]',        \n",
            "                                                                     'tf.repeat_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 2, 20000)             1026000   ['decoderlayer[0][0]']        \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60148768 (229.45 MB)\n",
            "Trainable params: 60148768 (229.45 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_heads = 8\n",
        "emd_dim = 512\n",
        "vocab_size = 20000\n",
        "seq_len = 2\n",
        "dense_dim = 2048\n",
        "num_layers = 1\n",
        "\n",
        "enc_inputs = Input(shape = (None,))\n",
        "enc_emb = Embeddings(vocab_size,seq_len,emd_dim)\n",
        "x = enc_emb(enc_inputs)\n",
        "enc_mask = enc_emb.compute_masks(enc_inputs)\n",
        "\n",
        "for _ in range(num_layers):\n",
        "  x = Encoderlayer(num_heads,emd_dim,dense_dim)(x,enc_mask)\n",
        "\n",
        "enc_output = x\n",
        "\n",
        "dec_inputs = Input(shape = (None,))\n",
        "dec_emb = Embeddings(vocab_size,2,emd_dim)\n",
        "x = dec_emb(dec_inputs)\n",
        "dec_mask = dec_emb.compute_masks(dec_inputs)\n",
        "\n",
        "\n",
        "for _ in range(num_layers):\n",
        "  x = Decoderlayer(num_heads,emd_dim,dense_dim)(x,enc_output,dec_mask)\n",
        "\n",
        "output = Dense(vocab_size , activation = 'softmax')(x)\n",
        "\n",
        "model = tf.keras.Model([enc_inputs,dec_inputs],output)\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer = Adam(),\n",
        "              loss = SparseCategoricalCrossentropy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.array([[1,2]])\n",
        "x2 = np.array([[3,4]])\n",
        "y = np.array([[4,5]])\n",
        "\n",
        "\n",
        "\n",
        "history=model.fit(\n",
        "    [x1,x2],y,\n",
        "    epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYNm18AZ0NhX",
        "outputId": "172d7b95-2e37-4d4d-9a5b-9d71ad5ce9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 10.0901\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 6.7956\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 6.8170\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.6319\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.7412\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.9335\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.1961\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.4893\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8634\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3753\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0536\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8738\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7832\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7390\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7173\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7061\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7002\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6969\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6950\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6939\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6932\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6928\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6925\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6923\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6921\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6921\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6920\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6919\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6919\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6918\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6918\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6918\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6917\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6917\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6917\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6916\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6916\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6915\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6915\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6915\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6914\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6913\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6913\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6912\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6911\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6910\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6909\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6908\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6907\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6906\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6904\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6902\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6900\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6898\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6896\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6893\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6890\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6887\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6884\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 1s 992ms/step - loss: 0.6878\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6873\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6868\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6864\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6860\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6852\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6840\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6824\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6808\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6812\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6999\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7418\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6920\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6905\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6861\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6924\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6808\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6933\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6867\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6839\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6899\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6797\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6848\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6795\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6750\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6754\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6646\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6587\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6301\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4992\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5203\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3763\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9089\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9582\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7716\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6855\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7698\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8344\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8031\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7265\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.array([[1,2]])\n",
        "x2 = np.array([[3,4]])\n",
        "\n",
        "print(np.argmax(model.predict([x1,x2])))"
      ],
      "metadata": {
        "id": "0XO9Q29n0j-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8dafdf-b317-4645-e271-fd8e0045828f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 153ms/step\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xl3prCgNTxqP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}